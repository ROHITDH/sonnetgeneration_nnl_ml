{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#sonnet generator\n",
    "#ada_lovelase_day_celebration\n",
    "#19/09/2020\n",
    "\n",
    "#importing lib/packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide data set\n",
    "with open('res/dataset.txt', 'r', encoding = 'utf8') as f:\n",
    "    data = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing is not need, since sonnet can't contain NaN\n",
    "#Analyise data set\n",
    "#so here we can observe average length of the sonnet interms of characters\n",
    "#\\n\\n is a differentiater\n",
    "sonnets = data.split('\\n\\n')\n",
    "sonnet_len = [len(sonnet) for sonnet in sonnets]\n",
    "\n",
    "plt.style.use('classic')\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot([i for i in range(1, len(sonnets)+1)], sonnet_len)\n",
    "plt.show()\n",
    "\n",
    "print('AVG len: %f' % np.mean(sonnet_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization of the data for the gen of new datas\n",
    "#max length of characters per sentence is approx. 40\n",
    "max_length_seq = 40\n",
    "step_size = 3\n",
    "sentences = []\n",
    "destin = []\n",
    "\n",
    "for i in range(0, len(data) - max_length_seq, step_size):\n",
    "    sentences.append(data[i:i + max_length_seq])\n",
    "    destin.append(data[max_length_seq + i])\n",
    "\n",
    "#all unique characters\n",
    "uniques = sorted(list(set(data)))\n",
    "\n",
    "#unique --> integer index\n",
    "unique_index = dict((unique , uniques.index(unique)) for unique in uniques)\n",
    "\n",
    "#creating numpy array to hold this vectorized data\n",
    "x = np.zeros((len(sentences),max_length_seq, len(uniques)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(uniques)), dtype=np.bool)\n",
    "for i, sentences in enumerate(sentences):\n",
    "    for j, unique in enumerate(sentences):\n",
    "        x[i, j, unique_index[unique]] = 1\n",
    "    y[i, unique_index[destin[i]]] = 1\n",
    "\n",
    "print(\"Size of training sequences:\", x.shape)\n",
    "print(\"Size of training targets:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of  a model and fitting it\n",
    "# output with softmax activation function\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_length_seq, len(uniques))))\n",
    "model.add(Dense(len(uniques), activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict probability --> newly created probability\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60 #100\n",
    "\n",
    "loss = []  # save model's loss\n",
    "\n",
    "#dir to store generated text\n",
    "base_dir = 'data_generated'\n",
    "if not os.path.isdir(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    #(if encountered with utf-9 than pass it)\n",
    "    try:\n",
    "        print(\"Epoch\", epoch)\n",
    "    #fit model with 1 epoch\n",
    "    #generate text give seed\n",
    "        history = model.fit(x, y, batch_size=128, epochs=1)\n",
    "        loss.append(history.history['loss'][0])\n",
    "    \n",
    "    #dir for each epoch\n",
    "        epoch_dir = os.path.join(base_dir, 'epoch_' + str(epoch))\n",
    "        if not os.path.isdir(epoch_dir):\n",
    "            os.mkdir(epoch_dir)\n",
    "    \n",
    "    #random seed feed into model and generate text\n",
    "        start_idx = np.random.randint(0, len(data) - max_length_seq - 1)\n",
    "        seed_text = data[start_idx:start_idx + max_length_seq]\n",
    "        for temp in [0.2, 0.5, 1.0, 1.3]:\n",
    "            data_generated = seed_text\n",
    "            temp_file = 'epoch' + str(epoch) + '_temp' + str(temp) + '.txt'\n",
    "            file = open(os.path.join(epoch_dir, temp_file), 'w')\n",
    "            file.write(data_generated)\n",
    "        \n",
    "        # generate 1 sonnet length chars\n",
    "            for i in range(625): #approx 625\n",
    "            # Vectorize\n",
    "                sampled = np.zeros((1, max_length_seq, len(uniques)))\n",
    "                for j, unique in enumerate(data_generated):\n",
    "                    sampled[0, j, unique_index[unique]] = 1.\n",
    "            #next unique\n",
    "                preds = model.predict(sampled, verbose=0)[0]\n",
    "                pred_idx = sample(preds, temperature=temp)\n",
    "                next_unique = uniques[pred_idx]\n",
    "            \n",
    "            #join unique to seed text\n",
    "                data_generated += next_unique\n",
    "                data_generated = data_generated[1:]\n",
    "            #text file\n",
    "                file.write(next_unique)\n",
    "            print('Temp ' + str(temp) + \" done.\")\n",
    "            file.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sonnet_gen_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
